% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_learner.R
\name{ml_learner}
\alias{ml_learner}
\title{Specify a Machine Learning Learner for Nuisance Models}
\usage{
ml_learner(
  method = c("ranger", "xgboost", "grf", "glmnet", "superlearner", "custom"),
  ...,
  fit_fun = NULL,
  predict_fun = NULL
)
}
\arguments{
\item{method}{Character string specifying the learner type:
\itemize{
\item \code{"ranger"}: Random forest via \code{\link[ranger:ranger]{ranger::ranger()}}
\item \code{"xgboost"}: Gradient boosting via \code{\link[xgboost:xgboost]{xgboost::xgboost()}}
\item \code{"grf"}: Generalized random forest via \code{\link[grf:regression_forest]{grf::regression_forest()}} or
\code{\link[grf:probability_forest]{grf::probability_forest()}}
\item \code{"glmnet"}: Regularized regression via \code{\link[glmnet:cv.glmnet]{glmnet::cv.glmnet()}}
\item \code{"superlearner"}: Ensemble via \code{\link[SuperLearner:SuperLearner]{SuperLearner::SuperLearner()}}
\item \code{"custom"}: User-supplied fit/predict functions
}}

\item{...}{Additional arguments passed to the fitting function.}

\item{fit_fun}{For \code{method = "custom"}, a function with signature
\verb{function(formula, data, family, ...)} that returns a fitted model object.}

\item{predict_fun}{For \code{method = "custom"}, a function with signature
\verb{function(object, newdata, ...)} that returns predicted probabilities.}
}
\value{
An object of class \code{ml_learner} containing the learner specification.
}
\description{
Creates a learner specification that can be passed to \code{propensity_model}
or \code{outcome_model} arguments in \code{\link[=cf_mse]{cf_mse()}}, \code{\link[=cf_auc]{cf_auc()}}, \code{\link[=cf_calibration]{cf_calibration()}},
and their transportability variants. When an \code{ml_learner} specification is
provided, cross-fitting is automatically used for valid inference.
}
\details{
\subsection{Supported Learners}{

\strong{ranger}: Fast random forest implementation. Key arguments:
\itemize{
\item \code{num.trees}: Number of trees (default: 500)
\item \code{mtry}: Number of variables to sample at each split
\item \code{min.node.size}: Minimum node size
}

\strong{xgboost}: Gradient boosting. Key arguments:
\itemize{
\item \code{nrounds}: Number of boosting rounds (default: 100)
\item \code{max_depth}: Maximum tree depth (default: 6)
\item \code{eta}: Learning rate (default: 0.3)
}

\strong{grf}: Generalized random forests with built-in honesty. Key arguments:
\itemize{
\item \code{num.trees}: Number of trees (default: 2000)
\item \code{honesty}: Whether to use honest estimation (default: TRUE)
}

\strong{glmnet}: Elastic net regularization with cross-validation. Key arguments:
\itemize{
\item \code{alpha}: Elastic net mixing parameter (0 = ridge, 1 = lasso, default: 1)
\item \code{nfolds}: Number of CV folds for lambda selection (default: 10)
}

\strong{superlearner}: Ensemble of multiple learners. Key arguments:
\itemize{
\item \code{SL.library}: Vector of learner names (default: \code{c("SL.glm", "SL.ranger")})
}
}
}
\examples{
\dontrun{
# Random forest for propensity score
cf_mse(
  Y = outcome, A = treatment, predictions = preds, data = df,
  propensity_formula = treatment ~ x1 + x2,
  propensity_model = ml_learner("ranger", num.trees = 500),
  cross_fit = TRUE
)

# XGBoost with custom parameters
cf_mse(
  Y = outcome, A = treatment, predictions = preds, data = df,
  propensity_formula = treatment ~ x1 + x2,
  propensity_model = ml_learner("xgboost", nrounds = 200, max_depth = 4),
  cross_fit = TRUE
)

# Custom learner
my_fit <- function(formula, data, family, ...) {
  glm(formula, data = data, family = binomial())
}
my_predict <- function(object, newdata, ...) {
  predict(object, newdata = newdata, type = "response")
}
cf_mse(
  ...,
  propensity_model = ml_learner("custom", fit_fun = my_fit,
                                 predict_fun = my_predict)
)
}
}
\seealso{
\code{\link[=cf_mse]{cf_mse()}}, \code{\link[=cf_auc]{cf_auc()}}, \code{\link[=cf_calibration]{cf_calibration()}}
}
